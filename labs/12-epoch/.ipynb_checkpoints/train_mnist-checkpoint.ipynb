{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import time\n",
    "import matplotlib\n",
    "from random import randint\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class one_layer_net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(one_layer_net , self).__init__()\n",
    "        self.linear_layer = nn.Linear( input_size, output_size , bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.linear_layer(x)\n",
    "        z = F.log_softmax(y, dim=1)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_layer_net(\n",
      "  (linear_layer): Linear(in_features=784, out_features=10, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net=one_layer_net(28*28,10)\n",
    "criterion = nn.NLLLoss()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "torch.Size([60000, 28, 28])\n",
      "torch.LongTensor\n",
      "torch.Size([60000])\n",
      "torch.FloatTensor\n",
      "torch.Size([10000, 28, 28])\n",
      "torch.LongTensor\n",
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "train_data=torch.load('../../data/mnist/data/train_data.pt')\n",
    "train_label=torch.load('../../data/mnist/data/train_label.pt')\n",
    "test_data=torch.load('../../data/mnist/data/test_data.pt')\n",
    "test_label=torch.load('../../data/mnist/data/test_label.pt')\n",
    "\n",
    "print(train_data.type())\n",
    "print(train_data.size())\n",
    "print(train_label.type())\n",
    "print(train_label.size())\n",
    "print(test_data.type())\n",
    "print(test_data.size())\n",
    "print(test_label.type())\n",
    "print(test_label.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 loss= 1.307476729258895\n",
      "1000 loss= 0.7538622219732497\n",
      "1500 loss= 0.6147295207027346\n",
      "2000 loss= 0.5614683861841914\n",
      "2500 loss= 0.44771240344282703\n",
      "3000 loss= 0.4720882098949514\n",
      "3500 loss= 0.44457570591452533\n",
      "4000 loss= 0.4651585184186115\n",
      "4500 loss= 0.4503104035903234\n",
      "5000 loss= 0.39758621981054604\n",
      "5500 loss= 0.4387986322023935\n",
      "6000 loss= 0.4834520030177664\n",
      "6500 loss= 0.4300082737702542\n",
      "7000 loss= 0.4314994654005905\n",
      "7500 loss= 0.3888902544539887\n",
      "8000 loss= 0.4216216577869054\n",
      "8500 loss= 0.3876423388075782\n",
      "9000 loss= 0.3725943948388449\n",
      "9500 loss= 0.444857981969275\n"
     ]
    }
   ],
   "source": [
    "lr=1e-2\n",
    "max_iter=10000\n",
    "running_loss=0\n",
    "print_every=500\n",
    "\n",
    "for iter in range(1,max_iter):\n",
    "    \n",
    "    idx=randint(0, train_data.size(0)-1)\n",
    "    input = Variable(train_data[idx].view(1,28*28))\n",
    "    label = Variable(torch.LongTensor([train_label[idx]]))\n",
    "\n",
    "    log_prob=net(input) \n",
    "    net.zero_grad()\n",
    "    loss = criterion(log_prob, label)\n",
    "    loss.backward()\n",
    "    running_loss = running_loss+ loss.data.item()    \n",
    "\n",
    "    for param in net.parameters():        \n",
    "        param.data -= lr * param.grad.data   \n",
    "        \n",
    "    if iter%print_every == 0:\n",
    "        lr=lr/2\n",
    "        print(iter, 'loss=', running_loss / print_every)\n",
    "        running_loss=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_of_correct_pred(scores, targets):\n",
    "    _, predicted = torch.max(scores, 1)\n",
    "    correct = (predicted == targets)\n",
    "    return correct.sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = 0.0\n",
    "correct_pred = 0\n",
    "\n",
    "for idx in range(test_data.size(0)):\n",
    "\n",
    "    input = Variable(test_data[idx].view(1,28*28))\n",
    "    label = Variable(torch.LongTensor([test_label[idx]]))\n",
    "\n",
    "    log_prob=net(input) \n",
    "    loss = criterion(log_prob, label)\n",
    "   \n",
    "    total_loss += loss.data.item()\n",
    "    correct_pred += nb_of_correct_pred(log_prob.data, label.data)\n",
    "    \n",
    "print('accuracy on test set= %s percent'%(correct_pred/test_data.size(0)) )\n",
    "print('loss on test set= %s percent'%(total_loss/test_data.size(0)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
